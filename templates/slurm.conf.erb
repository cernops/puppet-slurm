<%# slurm.conf.erb                                                           -%>
<%#                                                                          -%>
<%# Common SLURM parameters for db, head and worker node                     -%>
<%#                                                                          -%>
<%# version 20170331                                                         -%>
<%#                                                                          -%>
<%# Copyright (c) CERN, 2016-2017                                            -%>
<%# Authors: - Philippe Ganz <phganz@cern.ch>                                -%>
<%#          - Carolina Lindqvist <calindqv@cern.ch>                         -%>
<%# License: GNU GPL v3 or later.                                            -%>
<%#                                                                          -%>
# /etc/slurm/slurm.conf generated by puppet
#
# Configuration file containing common parameters for headnode and workernode.
#

ControlMachine=<%= @control_machine %>
#ControlAddr=
BackupController=<%= @backup_controller %>
#BackupAddr=

AuthType=auth/munge
#CheckpointType=checkpoint/none
CryptoType=crypto/openssl
#DisableRootJobs=NO
EnforcePartLimits=ALL
#Epilog=
#EpilogSlurmctld=
FirstJobId=1
#MaxJobId=999999
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
JobCheckpointDir=<%= @job_checkpoint_dir %>
JobCredentialPrivateKey=<%= @job_credential_private_key %>
JobCredentialPublicCertificate=<%= @job_credential_public_certificate %>
#JobFileAppend=0
JobRequeue=0
#JobSubmitPlugins=1
KillOnBadExit=1
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
MaxJobCount=<%= @max_job_count %>
#MaxStepCount=40000
MaxTasksPerNode=32
MpiDefault=pmi2
#MpiParams=ports=#-#
PluginDir=<%= @plugin_dir %>
PlugStackConfig=<%= @plug_stack_config %>
#PrivateData=jobs # TODO Should all users see all the jobs or the nodes currently down ?
ProctrackType=proctrack/cgroup
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
PropagateResourceLimitsExcept=ALL
RebootProgram=/usr/sbin/reboot
ReturnToService=1
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=<%= @slurmctld_port %>
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=<%= @slurmd_port %>
SlurmdSpoolDir=<%= @slurmd_spool_dir %>
SlurmUser=<%= @slurm_user %>
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation=<%= @state_save_location %>
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/cgroup
TaskPluginParam=Sched
#TaskProlog=
TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
TreeWidth=<%= (Math.sqrt(@amount_of_nodes).ceil) %> # Optimal system performance can typically be achieved if TreeWidth is set to the square root of the number of nodes in the cluster for systems having no more than 2500 nodes or the cube root for larger systems. ref https://slurm.schedmd.com/slurm.conf.html
UnkillableStepProgram=/etc/slurm/job_stuck_alert.sh
#UsePAM=0
VSizeFactor=200


# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=120
SlurmdTimeout=300
UnkillableStepTimeout=600
#VSizeFactor=0
Waittime=0


# SCHEDULING
DefMemPerCPU=4000
FastSchedule=1
MaxMemPerCPU=4000
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_CPU_Memory


# JOB PRIORITY
#PriorityFlags=
#PriorityType=priority/basic
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
#PriorityWeightPartition=
#PriorityWeightQOS=


# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
AccountingStorageHost=<%= @accounting_storage_host %>
#AccountingStorageLoc=<%= @accounting_storage_loc %>
AccountingStoragePass=/var/run/munge/munge.socket.2
AccountingStoragePort=<%= @accounting_storage_port %>
AccountingStorageType=accounting_storage/slurmdbd
#AccountingStorageUser=<%= @accounting_storage_user %>
#AccountingStoreJobComment=NO # YES
ClusterName=<%= @cluster_name %>
#DebugFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency=300
JobAcctGatherType=jobacct_gather/cgroup
#AcctGatherProfileType=acct_gather_profile/hdf5
AcctGatherEnergyType=acct_gather_energy/ipmi
AcctGatherNodeFreq=60
SlurmctldDebug=3
SlurmctldLogFile=<%= @slurmctld_log_file %>
SlurmdDebug=3
SlurmdLogFile=<%= @slurmd_log_file %>
# Note, these two were commented and the loglevel is named differently. Check if works.
#SlurmSchedLogFile=
#SlurmSchedLogLevel=


# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=

# COMPUTE NODES
<% @workernodes.each do |workernode| %>
<% workernode.each_pair do |key, value| -%>
<%= key %>=<%= value -%><%= ' '-%>
<% end %>
<% end %>

# PARTITIONS
<% @partitions.each do |partition| %>
<% partition.each_pair do |key,value| -%>
<%= key %>=<%= value -%><%= ' ' -%>
<% end %>
<% end %>
